{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX6A0vR-WAgq",
        "outputId": "752c52ff-ced0-4f9d-f5d5-b8159e31f120"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'text_summarization_project'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 30 (delta 10), reused 24 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (30/30), 36.47 KiB | 5.21 MiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/herrouzsalah/text_summarization_project.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd text_summarization_project/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WN-tclviXN9k",
        "outputId": "8de4aa9a-2dda-4f5b-8162-ef8f0fe71f87"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/text_summarization_project/text_summarization_project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arabert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnlTyr0gXt3X",
        "outputId": "6311b0e7-fb8f-45cd-8103-e2a94e135caa"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: arabert in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: PyArabic in /usr/local/lib/python3.10/dist-packages (from arabert) (0.6.15)\n",
            "Requirement already satisfied: farasapy in /usr/local/lib/python3.10/dist-packages (from arabert) (0.0.14)\n",
            "Requirement already satisfied: emoji==1.4.2 in /usr/local/lib/python3.10/dist-packages (from arabert) (1.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from farasapy->arabert) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from farasapy->arabert) (4.66.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from PyArabic->arabert) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2TokenizerFast, pipeline\n",
        "from transformers import GPT2LMHeadModel\n",
        "from arabert.aragpt2.grover.modeling_gpt2 import GPT2LMHeadModel\n",
        "from arabert.preprocess import ArabertPreprocessor"
      ],
      "metadata": {
        "id": "tsPpNrgdXt6H"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Complete this cell\n",
        "MODEL_NAME = \"aubmindlab/aragpt2-base\"\n",
        "arabert_prep = ArabertPreprocessor(model_name=MODEL_NAME)\n",
        "\n",
        "text=\"الجزائر بلد\"\n",
        "text_clean = arabert_prep.preprocess(text)\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(MODEL_NAME)\n",
        "generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "#feel free to try different decoding settings\n",
        "generation_pipeline(text,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    num_beams=10,\n",
        "    max_length=200,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty = 3.0,\n",
        "    no_repeat_ngram_size = 3)[0]['generated_text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "c-zfnvJuXt85",
        "outputId": "a72c8d2c-a09d-47d2-a3e5-d7d3ebc7a1fe"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at aubmindlab/aragpt2-base were not used when initializing GPT2LMHeadModel: ['ln_f.bias', 'ln_f.weight']\n",
            "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at aubmindlab/aragpt2-base and are newly initialized: ['emb_norm.bias', 'emb_norm.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'الجزائر بلد ،جب ان هي انها فيه انه بصورة الأسرةرم في جميع الى أنها أنون الممثلة بشكل هى و كافة ولا شبه الأصل عن علىن نصفبنأن شكل النظام عبر الاسرةالات هو تعتبر بأن العلامةإالإ أيضا القيمة الأم نظ ى والاسرة فهي إلى المستقبل فى الافراد والمدر وتلك وهو وكلذا ممثلةلت ايضا الأفراد نور هما الشكل شكله العائلةحيات وكذلك وهي يعتبر الاصل الملاكالا�ث ون جيهان عليه 27 مثل بأكمله كبير من كيت وتعتبر 7نت مجموع الدولة له وبقية كلهم 11الت وباقي حتى تماما ويتم الشعب الملك وج 2ش آ 23 2017 منذ ج كلهاتى نورا كما الله ال ربي لجميع مست لة نظاموا أنهالل� القطاع كثيرة البياناالس بالفعل كبيرةتي مريم مختلف نتمنى اوج البندالس علية علاء. تلك والو 3 31 عمر بالتأكيد وعله دائما اليينئين وعلىالعه ضوء تبارك السيدةانته والدكتورة عموما مجتمعة معينينا شعب يب الل لذلك 1 بواسطة وبعض نوفبأ دليلاات لمختلف وشبه عبد بعض ماوين بالاضافة لغز فيصل الو بما نه ء انهماأت است بالتالي الاطفال الامة 26 لكل يعكس'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCikeyS7c26x",
        "outputId": "66396b9d-2131-495d-9b69-7980a92aeeb2"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(64000, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (emb_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=64000, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from src.utils_data import *\n",
        "from src.utils_tokenizer import *\n",
        "from src.train import *"
      ],
      "metadata": {
        "id": "ZGjFT288XuB6"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 512\n",
        "sum_length = 150\n",
        "split_probability = 0.9"
      ],
      "metadata": {
        "id": "BMKOqmtvXuE9"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, val, test = process_data(\"/content/text_summarization_project/data/arabic_texts_summaries.csv\",max_length , sum_length, split_probability)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zwv763PyXuHA",
        "outputId": "9314a8dc-f24f-4839-de08-18aaf0345dff"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train size: 5\n",
            "val size: 22\n",
            "test size: 23\n",
            "test head:\n",
            "                                                 text  \\\n",
            "29  تدور أحداث هذا النص حول مغامرة في الجبال. يبدأ...   \n",
            "\n",
            "                                  summary  text_len  \n",
            "29  تجربة تسلق جبال شاهقة واكتشاف الطبيعة        49  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add token to AraGPT2 tokenizer\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('aubmindlab/aragpt2-base')\n",
        "\n",
        "special_tokens = {\n",
        "    'bos_token': '<BOS>',  # Beginning of Sentence token\n",
        "    'eos_token': '<EOS>',  # End of Sentence token\n",
        "    'pad_token': '<PAD>',  # Padding token\n",
        "    'additional_special_tokens': ['<SEP>']  # Separator token, if needed\n",
        "}\n",
        "tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "print('tokenizer len: {}'.format(len(tokenizer)))\n",
        "\n",
        "ignore_idx = tokenizer.pad_token_id\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiyrTqFsXuI8",
        "outputId": "c98eb4b0-02ff-4742-8bc2-f8263baba99b"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer len: 64004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Special tokens added:\")\n",
        "print(\"BOS token ID:\", tokenizer.convert_tokens_to_ids('<BOS>'))\n",
        "print(\"EOS token ID:\", tokenizer.convert_tokens_to_ids('<EOS>'))\n",
        "print(\"PAD token ID:\", tokenizer.convert_tokens_to_ids('<PAD>'))\n",
        "print(\"SEP token ID:\", tokenizer.convert_tokens_to_ids('<SEP>'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dGkmsW5lM0q",
        "outputId": "af9b92ec-6ab7-4af6-fe22-9391f816adc4"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Special tokens added:\n",
            "BOS token ID: 64000\n",
            "EOS token ID: 64001\n",
            "PAD token ID: 64002\n",
            "SEP token ID: 64003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: apply tokenizer\n",
        "import os\n",
        "\n",
        "tokenizer_dir =\"tokenizer_path_save\"\n",
        "if not os.path.exists(tokenizer_dir):\n",
        "  os.makedirs(tokenizer_dir) # Create output directory if needed\n",
        "\n",
        "max_seq_len = 768\n",
        "tokenizer.save_pretrained(tokenizer_dir)\n",
        "tokenizer_len = len(tokenizer)\n",
        "print('ignore_index: {}'.format(tokenizer.pad_token_id))\n",
        "print('max_len: {}'.format(max_seq_len))\n",
        "\n",
        "train, val, test = tokenize_dataset(tokenizer, train, val, test, max_seq_len)# Fix tokenize_dataset function in utils_tokenizer and call it\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8Fbsrt7XuK1",
        "outputId": "380e4cd6-3922-4a0a-875b-3ab531bdc367"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ignore_index: 64002\n",
            "max_len: 768\n",
            "Tokenized DataFrame head:\n",
            "                                                 text  \\\n",
            "9   تدور أحداث هذا النص حول تجربة في مطعم جديد. يب...   \n",
            "15  تدور أحداث هذا النص حول رحلة في الصحراء. يبدأ ...   \n",
            "6   تدور أحداث هذا النص حول حفلة في القرية. يبدأ ا...   \n",
            "25  تدور أحداث هذا النص حول يوم دراسي. يبدأ النص ب...   \n",
            "8   تدور أحداث هذا النص حول قصة حب في المدينة. يبد...   \n",
            "\n",
            "                                              summary  text_len  \\\n",
            "9                تذوق أطباق عالمية في مطعم جديد وأنيق        52   \n",
            "15  رحلة مثيرة في الصحراء تكشف جمال الطبيعة الصحراوية        49   \n",
            "6   حفلة تراثية في قرية تظهر العادات والتقاليد الم...        49   \n",
            "25               يوم مليء بالتعلم والتفاعل في المدرسة        46   \n",
            "8        قصة حب مؤثرة تدور أحداثها في المدينة الصاخبة        52   \n",
            "\n",
            "                      encodings  \n",
            "9   [input_ids, attention_mask]  \n",
            "15  [input_ids, attention_mask]  \n",
            "6   [input_ids, attention_mask]  \n",
            "25  [input_ids, attention_mask]  \n",
            "8   [input_ids, attention_mask]  \n",
            "Tokenized DataFrame head:\n",
            "                                                 text  \\\n",
            "0   تدور أحداث هذا النص حول حفلة في القرية. يبدأ ا...   \n",
            "14  تدور أحداث هذا النص حول تجربة في مطعم جديد. يب...   \n",
            "47  تدور أحداث هذا النص حول رحلة في الصحراء. يبدأ ...   \n",
            "20  تدور أحداث هذا النص حول يوم في السوق. يبدأ الن...   \n",
            "37  تدور أحداث هذا النص حول رحلة بحرية. يبدأ النص ...   \n",
            "\n",
            "                                              summary  text_len  \\\n",
            "0   حفلة تراثية في قرية تظهر العادات والتقاليد الم...        49   \n",
            "14               تذوق أطباق عالمية في مطعم جديد وأنيق        52   \n",
            "47  رحلة مثيرة في الصحراء تكشف جمال الطبيعة الصحراوية        49   \n",
            "20        يوم حافل بالتسوق واستكشاف الأسواق التقليدية        49   \n",
            "37                    مغامرة بحرية تستكشف عجائب البحر        46   \n",
            "\n",
            "                      encodings  \n",
            "0   [input_ids, attention_mask]  \n",
            "14  [input_ids, attention_mask]  \n",
            "47  [input_ids, attention_mask]  \n",
            "20  [input_ids, attention_mask]  \n",
            "37  [input_ids, attention_mask]  \n",
            "Tokenized DataFrame head:\n",
            "                                                 text  \\\n",
            "29  تدور أحداث هذا النص حول مغامرة في الجبال. يبدأ...   \n",
            "42  تدور أحداث هذا النص حول حفلة في القرية. يبدأ ا...   \n",
            "19  تدور أحداث هذا النص حول تجربة في مطعم جديد. يب...   \n",
            "23  تدور أحداث هذا النص حول زيارة لمعلم تاريخي. يب...   \n",
            "48  تدور أحداث هذا النص حول يوم في السوق. يبدأ الن...   \n",
            "\n",
            "                                              summary  text_len  \\\n",
            "29              تجربة تسلق جبال شاهقة واكتشاف الطبيعة        49   \n",
            "42  حفلة تراثية في قرية تظهر العادات والتقاليد الم...        49   \n",
            "19               تذوق أطباق عالمية في مطعم جديد وأنيق        52   \n",
            "23               جولة في معلم تاريخي تعكس عظمة الماضي        49   \n",
            "48        يوم حافل بالتسوق واستكشاف الأسواق التقليدية        49   \n",
            "\n",
            "                      encodings  \n",
            "29  [input_ids, attention_mask]  \n",
            "42  [input_ids, attention_mask]  \n",
            "19  [input_ids, attention_mask]  \n",
            "23  [input_ids, attention_mask]  \n",
            "48  [input_ids, attention_mask]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate train/val/test files\n",
        "#save tokenized data\n",
        "out_dir=\"tokenizer_data\"\n",
        "processed_set= \"dataset\"\n",
        "data_dir = os.path.join(out_dir, processed_set)\n",
        "if not os.path.exists(data_dir):\n",
        "  os.makedirs(data_dir) # Create output directory if needed\n",
        "train_file = os.path.join(data_dir,\"train.csv\")\n",
        "train.to_csv(train_file, index=False)\n",
        "\n",
        "val_file = os.path.join(data_dir,\"val.csv\")\n",
        "val.to_csv(val_file, index=False)\n",
        "\n",
        "test_file = os.path.join(data_dir,\"test.csv\")\n",
        "test.to_csv(test_file, index=False)\n",
        ""
      ],
      "metadata": {
        "id": "af7OKO5zXuNI"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Visualize train and explain each column"
      ],
      "metadata": {
        "id": "GRgcrOJdYZwh"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = os.path.join(data_dir, \"train.csv\")\n",
        "train_df = pd.read_csv(train_file)\n",
        "print(train_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DfVBNS5Y5Bd",
        "outputId": "ac4e270e-d65e-4603-aa81-c334188f1f6e"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   text_len                                          encodings\n",
            "0        52  {'input_ids': tensor([[64000,  8908,  5368,   ...\n",
            "1        49  {'input_ids': tensor([[64000,  8908,  5368,   ...\n",
            "2        49  {'input_ids': tensor([[64000,  8908,  5368,   ...\n",
            "3        46  {'input_ids': tensor([[64000,  8908,  5368,   ...\n",
            "4        52  {'input_ids': tensor([[64000,  8908,  5368,   ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Data Loaders\n",
        "# Fix code in utils_data.py\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "import pandas as pd\n",
        "\n",
        "train_dataset, val_dataset = get_gpt2_dataset(train, val)\t\t# call function get_gpt2_dataset\n",
        "#b = train_dataset.__getitem__(1) # check one data row\n",
        "#print(b)\n",
        "print(train_dataset.sum_idx[2])\n",
        "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size = 1)\n",
        "val_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset), batch_size = 1)\n",
        "\n",
        "train_loader_len = len(train_dataloader)\n",
        "print('Number of batches in training data loader:', train_loader_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjrue-htYZzH",
        "outputId": "fc47eac4-98d6-4312-fd46-48b424aeb288"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49\n",
            "Number of batches in training data loader: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"out_dir\": \"/content/text_summarization_project/models\",\n",
        "    \"training_models\": \"gpt2_model\",\n",
        "    \"final_model\": \"gpt2_finetuned_final\",\n",
        "    \"learning_rate\": 5e-5,\n",
        "    #\"num_train_epochs\": 3,\n",
        "    #\"batch_size\": 16\n",
        "}\n",
        "\n",
        "# fine tune pretrained model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_dir =  'aubmindlab/aragpt2-base'\n",
        "\n",
        "train = Train(device, model_dir, tokenizer_len, ignore_idx, train_loader_len, config)\n",
        "train.train_model(train_dataloader, val_dataloader)\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "deWxhNoZYZ1r",
        "outputId": "dea78e48-152d-42ff-b532-1735227e8614"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 1/50 ========\n",
            "Batch 0: {'text': tensor([[[64000,  8908,  5368,   542,  3499,  1027,  6205,   305,  7427,    17,\n",
            "           5688,  3499, 12594, 22409, 39915, 20162,  1366, 10279,   152,   238,\n",
            "            952, 17073,  1464,   879,  5127, 34742, 22933,   468, 24025,  4777,\n",
            "          15437,   305,  6205,   305,  7427,    17, 12531,  2235,  6452,  6258,\n",
            "            599, 13204, 41148, 12213,  3600, 22610, 10768,   534,   226, 13456,\n",
            "            305,  7427,   152,   238,  1587,  4801,  8044,  6618, 13064,  3151,\n",
            "            308,  6452,    17,   224, 64003,  6205, 13798,   305,  7427,  8654,\n",
            "           3655,  7241, 22810,   224, 64001, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002,\n",
            "          64002, 64002, 64002, 64002, 64002, 64002, 64002, 64002]]]), 'mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "          1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0]]]), 's_idx': tensor([51])}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-200-4b3153aaeb7d>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text = torch.tensor(encoding['input_ids'], dtype=torch.long)  # Extract input_ids from encodings\n",
            "<ipython-input-200-4b3153aaeb7d>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  attn_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)  # Extract attention_mask from encodings\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected input batch_size (768) to match target batch_size (767).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-210-2adeac302cbf>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-201-febd63e0b601>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'======== Epoch {epoch_i + 1}/{epochs} ========'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Average training loss: {avg_train_loss:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-201-febd63e0b601>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(self, train_dataloader)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mshift_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_train_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshift_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1189\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3103\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3104\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (768) to match target batch_size (767)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import GPT2LMHeadModel, GPT2Config, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import os\n",
        "import time\n",
        "from torch.optim import AdamW\n",
        "\n",
        "class Train(object):\n",
        "    def __init__(self, device, model_path, tokenizer_len, ignore_index, train_loader_len, config):\n",
        "        self.device = device\n",
        "        self.config = config\n",
        "        # Define paths for saving models\n",
        "        self.final_model = os.path.join(config['out_dir'], config['final_model'])\n",
        "        if not os.path.exists(self.final_model):\n",
        "            os.makedirs(self.final_model)\n",
        "\n",
        "        self.training_models = os.path.join(config['out_dir'], config['training_models'])\n",
        "        if not os.path.exists(self.training_models):\n",
        "            os.makedirs(self.training_models)\n",
        "\n",
        "\n",
        "        # Load model and tokenizer\n",
        "        self.configuration = GPT2Config.from_pretrained(model_path, output_hidden_states=False)\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_path, config=self.configuration)\n",
        "        self.model.resize_token_embeddings(tokenizer_len)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Initialize optimizer and loss function\n",
        "        self.optimizer = AdamW(self.model.parameters(), lr=self.config.get('learning_rate', 5e-5))\n",
        "        #self.optimizer = AdamW(self.model.parameters(), lr=5e-4, eps=1e-8)\n",
        "        self.loss_fct = CrossEntropyLoss(ignore_index=ignore_index)  # Ignores padding token for loss calculation\n",
        "\n",
        "        self.gradient_accumulation_steps = 32\n",
        "        self.max_grad_norm = 1\n",
        "\n",
        "        # Create the learning rate scheduler\n",
        "        self.scheduler = get_linear_schedule_with_warmup(\n",
        "            self.optimizer,\n",
        "            num_warmup_steps=int(1e2),\n",
        "            num_training_steps=train_loader_len * 50\n",
        "        )\n",
        "\n",
        "    def process_train_batch(self, batch):\n",
        "        inputs, labels = batch['text'].to(self.device), batch['text'].to(self.device)\n",
        "        logits = self.model(input_ids=inputs)[0]\n",
        "\n",
        "        # Shift logits and labels to only consider the loss on the summary\n",
        "        shift_logits = logits[..., 1:].contiguous()\n",
        "        shift_labels = labels[..., 1:].contiguous()\n",
        "\n",
        "        return shift_logits, shift_labels\n",
        "\n",
        "    def process_eval_batch(self, batch):\n",
        "        inputs, labels = batch['text'].to(self.device), batch['text'].to(self.device)\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(input_ids=inputs)[0]\n",
        "\n",
        "            # Shift logits and labels to only consider the loss on the summary\n",
        "            shift_logits = logits[..., 1:].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "\n",
        "        return shift_logits, shift_labels\n",
        "    def train_loop(self, train_dataloader):\n",
        "        self.model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            print(f\"Batch {step}: {batch}\")  # Debug print\n",
        "\n",
        "            self.model.zero_grad()  # put all gradients to zero\n",
        "            shift_logits, shift_labels = self.process_train_batch(batch)\n",
        "\n",
        "            loss = self.loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "            loss = loss / self.gradient_accumulation_steps\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
        "            self.optimizer.step()\n",
        "            self.scheduler.step()\n",
        "            batch_loss = loss.item()\n",
        "            total_train_loss += batch_loss\n",
        "\n",
        "        return total_train_loss\n",
        "\n",
        "\n",
        "\n",
        "    def eval_loop(self, val_dataloader):\n",
        "        self.model.eval()\n",
        "        total_eval_loss = 0\n",
        "\n",
        "        for step, batch in enumerate(val_dataloader):\n",
        "            shift_logits, shift_labels = self.process_eval_batch(batch)\n",
        "            loss = self.loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "            batch_loss = loss.item()\n",
        "            total_eval_loss += batch_loss\n",
        "\n",
        "        return total_eval_loss\n",
        "\n",
        "    def average_loss(self, total_loss, size):\n",
        "        return total_loss / size\n",
        "\n",
        "    def save_model(self, path):\n",
        "        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model\n",
        "        model_to_save.save_pretrained(path)\n",
        "\n",
        "    def model_params(self):\n",
        "        name = \"params_\" + str(int(time.time())) + \".txt\"\n",
        "        file = os.path.join(self.config.out_dir, name)\n",
        "        params = list(self.model.named_parameters())\n",
        "        with open(file, \"w\") as f:\n",
        "            f.write('The model has {:} different named parameters.\\n'.format(len(params)))\n",
        "            f.write('\\n==== Embedding Layer ====\\n')\n",
        "            for p in params[0:2]:\n",
        "                f.write(\"{:<55} {:>12}\\n\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "            f.write('\\n==== First Transformer ====\\n')\n",
        "            for p in params[2:14]:\n",
        "                f.write(\"{:<55} {:>12}\\n\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "            f.write('\\n==== Output Layer ====\\n')\n",
        "            for p in params[-2:]:\n",
        "                f.write(\"{:<55} {:>12}\\n\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "    def train_model(self, train_dataloader, val_dataloader):\n",
        "        epochs = 50\n",
        "        for epoch_i in range(epochs):\n",
        "            print(f'======== Epoch {epoch_i + 1}/{epochs} ========')\n",
        "\n",
        "            total_train_loss = self.train_loop(train_dataloader)\n",
        "            avg_train_loss = self.average_loss(total_train_loss, len(train_dataloader))\n",
        "            print(f\"  Average training loss: {avg_train_loss:.2f}\")\n",
        "\n",
        "            total_eval_loss = self.eval_loop(val_dataloader)\n",
        "            avg_val_loss = self.average_loss(total_eval_loss, len(val_dataloader))\n",
        "            print(f\"  Valid. Loss: {avg_val_loss:.2f}\")\n",
        "\n",
        "            if (epoch_i + 1) % 10 == 0:\n",
        "                print(f'Saving model for epoch {epoch_i + 1}')\n",
        "                path = os.path.join(self.training_models, str(epoch_i + 1))\n",
        "                self.save_model(path)\n",
        "\n",
        "        print(\"Training complete!\")\n",
        "        self.save_model(self.final_model)\n"
      ],
      "metadata": {
        "id": "apqu0vPqoRNZ"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Utility functions for data manipulation and preparation\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
        "from torchvision import transforms, utils\n",
        "def tokenize_text(tokenizer, df, max_len):\n",
        "    df['encodings'] = df.apply(lambda x: apply_tokenizer(tokenizer, x['text'], x['summary'], max_len), axis=1)\n",
        "    print(f\"Tokenized DataFrame head:\\n{df.head()}\")  # Debug print\n",
        "\n",
        "    del df['text']\n",
        "    del df['summary']\n",
        "\n",
        "    return df\n",
        "\n",
        "# Dataset definition\n",
        "class GPT2Dataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.encodings = df['encodings'].to_list()\n",
        "        self.sum_idx = df['text_len'].to_list()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sum_idx)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.encodings[idx]\n",
        "        if 'input_ids' not in encoding or 'attention_mask' not in encoding:\n",
        "            raise KeyError(\"Encoding missing required keys: 'input_ids' or 'attention_mask'\")\n",
        "\n",
        "        # Convert lists to tensors\n",
        "        text = torch.tensor(encoding['input_ids'], dtype=torch.long)  # Extract input_ids from encodings\n",
        "        attn_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)  # Extract attention_mask from encodings\n",
        "        s_idx = self.sum_idx[idx] + 2  # Adjust index for BOS and EOS tokens\n",
        "\n",
        "        return {'text': text, 'mask': attn_mask, 's_idx': s_idx}\n",
        "\n",
        "\n",
        "def get_gpt2_dataset(train, val):\n",
        "\n",
        "  train_dataset = GPT2Dataset(train)\n",
        "  val_dataset = GPT2Dataset(val)\n",
        "\n",
        "  return train_dataset, val_dataset\n",
        "\n",
        "\n",
        "def short_text(text, max_len):\n",
        "\ttext = text.split()\n",
        "\t#print(len(text))\n",
        "\t#len= len-1\n",
        "\ts_text = text[:max_len]\n",
        "\ts_text = ' '.join(s_text)\n",
        "\treturn  s_text\n",
        "\n",
        "\n",
        "def process_dataframe(df, max_text, max_sum):\n",
        "\n",
        "\tdf['text'] = df['text'].apply(lambda x: short_text(x, max_text))\n",
        "\tdf['summary'] = df['summary'].apply(lambda x: short_text(x, max_sum))\n",
        "\tdf['text_len'] = df['text'].apply(lambda x: len(x.split()))\n",
        "\t#print(df['summary'].str.split().str.len())\n",
        "\t#df['summary'] = df['summary'].apply(lambda x: ' <CLS> ' + x) # do this step in data loader\n",
        "\t#df['ts'] = df[['text', 'summary']].apply(lambda x: ''.join(x), axis=1)\n",
        "\t#print(df['ts'].str.split().str.len())\n",
        "\n",
        "\treturn df\n",
        "\n",
        "def split_data (df, sr):\n",
        "\ttrain, val_test = train_test_split(df,test_size=sr)\n",
        "\tval, test = train_test_split(val_test,test_size=0.5)\n",
        "\treturn train, val, test\n",
        "\n",
        "def process_data(file, max_text, max_sum, sr):\n",
        "\n",
        "\t# load into a data frame\n",
        "\tdf = pd.read_csv(file)\n",
        "\tdf = process_dataframe(df, max_text, max_sum)\n",
        "\ttrain, val, test = split_data(df, sr)\n",
        "\tprint('train size: {}'.format(len(train)))\n",
        "\tprint('val size: {}'.format(len(val)))\n",
        "\tprint('test size: {}'.format(len(test)))\n",
        "\tprint('test head:\\n{}'.format(test.head(1)))\n",
        "\n",
        "\treturn train, val, test\n",
        ""
      ],
      "metadata": {
        "id": "Zedy0h_CtofA"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions for tokenization\n",
        "def apply_tokenizer(tokenizer, text, summary, max_len):\n",
        "\t# TODO: Complete this function\n",
        "\t# Concatenate text and summary and in-between add a special token\n",
        "\t# <BOS> is beginning of sentence\n",
        "\t# <EOS> is end of sentence\n",
        "  combined_text = f'<BOS> {text} <SEP> {summary} <EOS>'\n",
        "  encoding = tokenizer(combined_text,\n",
        "                         truncation=True,\n",
        "                         max_length=max_len,\n",
        "                         padding=\"max_length\",\n",
        "                         return_tensors='pt')\n",
        "\n",
        "  return encoding\n",
        "\t#return tokenizer('<BOS>'  '<EOS>', truncation=True, max_length=max_len, padding=\"max_length\")\n",
        "\n",
        "def tokenize_text(tokenizer, df, max_len):\n",
        "    df['encodings'] = df.apply(lambda x : apply_tokenizer(tokenizer, x['text'], x['summary'], max_len), axis=1)\n",
        "    #encodings is dict type: contains input_ids and attention_mas\n",
        "    df['input_ids'] = df['encodings'].apply(lambda x: x['input_ids'].squeeze().tolist())\n",
        "    df['attention_mask'] = df['encodings'].apply(lambda x: x['attention_mask'].squeeze().tolist())\n",
        "\n",
        "    del df['text']\n",
        "    del df['summary']\n",
        "    del df['encodings']\n",
        "\n",
        "    return df\n",
        "\n",
        "def tokenize_dataset(tokenizer, train, val, test, max_len):\n",
        "\ttrain = tokenize_text(tokenizer, train, max_len)\n",
        "\tval = tokenize_text(tokenizer, val, max_len)\n",
        "\ttest = tokenize_text(tokenizer, test, max_len)\n",
        "\treturn train, val, test"
      ],
      "metadata": {
        "id": "CxL7P7tU6io7"
      },
      "execution_count": 217,
      "outputs": []
    }
  ]
}